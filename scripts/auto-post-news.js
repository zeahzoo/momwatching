#!/usr/bin/env node

const fs = require('fs').promises;
const path = require('path');
const { execSync } = require('child_process');

// Configuration
const PIXABAY_API_KEY = '46430210-e974faa2be384e27927d08033';
const NEWS_KEYWORDS = ['ì…ì‹œ', 'ê³ ë“±í•™êµ', 'ëŒ€ì…', 'ìˆ˜ì‹œ', 'ì •ì‹œ', 'ì„œìš¸ëŒ€', 'ì§„í•™'];
const NEWS_SOURCES = [
  { name: 'ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ', url: 'https://www.veritas-a.com/' },
  { name: 'ì—°í•©ë‰´ìŠ¤', url: 'https://www.yna.co.kr/' }
];

/**
 * Fetch news from web sources using web_search
 */
async function fetchNews() {
  console.log('ğŸ” Searching for education news...');
  
  // For now, we'll create a template. In production, this would use web scraping
  // or RSS feeds from ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ and ì—°í•©ë‰´ìŠ¤
  
  const keyword = NEWS_KEYWORDS[Math.floor(Math.random() * NEWS_KEYWORDS.length)];
  const searchQuery = `${keyword} ë‰´ìŠ¤ 2026`;
  
  console.log(`Search query: ${searchQuery}`);
  
  // In a real implementation, you would:
  // 1. Use web_search tool or RSS feed parser
  // 2. Scrape article content
  // 3. Filter by relevance
  
  return {
    keyword,
    searchQuery,
    foundArticles: [] // Placeholder for actual scraped articles
  };
}

/**
 * Generate article using GPT (mock for now - would use OpenAI API)
 */
async function generateArticle(keyword) {
  console.log(`âœï¸  Generating article for keyword: ${keyword}`);
  
  const timestamp = new Date().toISOString();
  const slug = `${keyword}-news-${Date.now()}`;
  
  // Template article (in production, use GPT API)
  const article = {
    id: `news-${Date.now()}`,
    title: `2026 ${keyword} ìµœì‹  ë™í–¥ - ê³ ë“±í•™êµ ì…ì‹œ ë‰´ìŠ¤`,
    slug: slug,
    date: timestamp,
    summary: `ìµœê·¼ ${keyword} ê´€ë ¨ ì£¼ìš” ë³€í™”ê°€ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìƒê³¼ í•™ë¶€ëª¨ë“¤ì´ ì•Œì•„ì•¼ í•  í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì…ì‹œ ì „ëµì€ momwatching.comì—ì„œ í™•ì¸í•˜ì„¸ìš”.`,
    content: `# ${keyword} ì£¼ìš” ë³€í™” ë‚´ìš©

ìµœê·¼ êµìœ¡ë¶€ì—ì„œ ë°œí‘œí•œ ${keyword} ê´€ë ¨ ì •ì±… ë³€í™”ê°€ í•™ìƒê³¼ í•™ë¶€ëª¨ë“¤ì˜ í° ê´€ì‹¬ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.

## ì£¼ìš” ë‚´ìš©

ì´ë²ˆ ë°œí‘œì˜ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **ì •ì±… ë°©í–¥**: ${keyword}ì™€ ê´€ë ¨ëœ ìƒˆë¡œìš´ ë°©í–¥ì„±ì´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.
2. **ì ìš© ì‹œê¸°**: 2026í•™ë…„ë„ë¶€í„° ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©ë  ì˜ˆì •ì…ë‹ˆë‹¤.
3. **ì˜í–¥ ë²”ìœ„**: ì „êµ­ ê³ ë“±í•™êµì™€ ëŒ€í•™ ì…ì‹œ ì „ë°˜ì— ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

## í•™ìƒ ë° í•™ë¶€ëª¨ ëŒ€ì‘ ë°©ì•ˆ

ì „ë¬¸ê°€ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ëŒ€ì‘ì´ í•„ìš”í•˜ë‹¤ê³  ì¡°ì–¸í•©ë‹ˆë‹¤:

- ë³€í™”ëœ ì •ì±…ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì¤€ë¹„
- í•™êµë³„ ì…ì‹œ ì‹¤ì ê³¼ ì „ëµ ë¹„êµ ë¶„ì„
- ì „ë¬¸ê°€ ìƒë‹´ì„ í†µí•œ ë§ì¶¤í˜• ì§„í•™ ì„¤ê³„

## ë” ì•Œì•„ë³´ê¸°

ì „êµ­ ê³ ë“±í•™êµì˜ ì„œìš¸ëŒ€ ì§„í•™ ì‹¤ì ê³¼ ìƒì„¸í•œ ì…ì‹œ ì •ë³´ëŠ” momwatching.comì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`,
    image: '/news/images/default.jpg',
    keywords: [keyword, 'ì…ì‹œ', 'ê³ ë“±í•™êµ', '2026'],
    source: 'ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ'
  };
  
  return article;
}

/**
 * Fetch image from Pixabay
 */
async function fetchImage(keyword) {
  console.log(`ğŸ–¼ï¸  Fetching image for: ${keyword}`);
  
  try {
    const searchTerm = 'education students study'; // Generic education image
    const url = `https://pixabay.com/api/?key=${PIXABAY_API_KEY}&q=${encodeURIComponent(searchTerm)}&image_type=photo&per_page=3&safesearch=true`;
    
    const response = await fetch(url);
    const data = await response.json();
    
    if (data.hits && data.hits.length > 0) {
      const image = data.hits[0];
      console.log(`âœ… Found image: ${image.pageURL}`);
      return image.webformatURL;
    }
  } catch (error) {
    console.error('Error fetching image from Pixabay:', error);
  }
  
  return '/news/images/default.jpg';
}

/**
 * Update news.json
 */
async function updateNewsDatabase(article) {
  console.log('ğŸ’¾ Updating news database...');
  
  const newsFilePath = path.join(__dirname, '..', 'data', 'news.json');
  
  let articles = [];
  try {
    const content = await fs.readFile(newsFilePath, 'utf8');
    articles = JSON.parse(content);
  } catch (error) {
    console.log('Creating new news.json file...');
  }
  
  articles.unshift(article);
  
  // Keep only last 100 articles
  if (articles.length > 100) {
    articles = articles.slice(0, 100);
  }
  
  await fs.writeFile(newsFilePath, JSON.stringify(articles, null, 2));
  console.log('âœ… News database updated');
}

/**
 * Commit and push to Git
 */
function commitAndPush(articleTitle) {
  console.log('ğŸ“¤ Committing and pushing to Git...');
  
  try {
    const rootDir = path.join(__dirname, '..');
    
    execSync('git add data/news.json', { cwd: rootDir });
    execSync(`git commit -m "ìë™ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸: ${articleTitle}"`, { cwd: rootDir });
    execSync('git push origin main', { cwd: rootDir });
    
    console.log('âœ… Successfully pushed to Git');
    console.log('ğŸš€ Vercel will auto-deploy the changes');
  } catch (error) {
    console.error('âŒ Error pushing to Git:', error.message);
  }
}

/**
 * Main function
 */
async function main() {
  console.log('ğŸ¤– Starting automated news posting...');
  console.log(`ğŸ“… Time: ${new Date().toLocaleString('ko-KR')}`);
  console.log('â”€'.repeat(50));
  
  try {
    // 1. Fetch news
    const newsData = await fetchNews();
    
    // 2. Generate article
    const article = await generateArticle(newsData.keyword);
    
    // 3. Fetch image
    const imageUrl = await fetchImage(newsData.keyword);
    article.image = imageUrl;
    
    // 4. Update database
    await updateNewsDatabase(article);
    
    // 5. Git commit & push
    commitAndPush(article.title);
    
    console.log('â”€'.repeat(50));
    console.log('âœ… News posting completed successfully!');
    console.log(`ğŸ“ Title: ${article.title}`);
    console.log(`ğŸ”— Slug: ${article.slug}`);
  } catch (error) {
    console.error('âŒ Error in main process:', error);
    process.exit(1);
  }
}

// Run if called directly
if (require.main === module) {
  main();
}

module.exports = { main };
